{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d13300-61e1-43c5-9158-745165bde4d0",
   "metadata": {},
   "source": [
    "<h4 style=\"color:#1a73e8;\">2.3.5 Encoding Categorical Variables: A Comprehensive Guide</h4>\n",
    "\n",
    "Machine learning algorithms—whether linear regression, decision trees, or deep neural networks—are fundamentally mathematical constructs that operate on numerical data. **Categorical variables**, which represent qualitative attributes (e.g., \"country\", \"product type\", or \"education level\"), cannot be directly processed by these models. Therefore, transforming such variables into a numerical format—known as **encoding**—is a critical preprocessing step.\n",
    "\n",
    "However, **not all encodings are equal**. The choice of encoding method profoundly impacts model performance, interpretability, and generalization. A naive approach may introduce artificial relationships, inflate dimensionality, or leak target information. This section explores the taxonomy of categorical variables, examines mainstream and advanced encoding techniques, and provides actionable guidelines for real-world applications.\n",
    "\n",
    "---\n",
    "\n",
    "### **Understanding Categorical Variable Types**\n",
    "\n",
    "Before selecting an encoding method, it is essential to correctly classify the variable:\n",
    "\n",
    "1. **Nominal Variables**:  \n",
    "   These categories have **no intrinsic order or hierarchy**. Examples include:\n",
    "   - `[\"Red\", \"Blue\", \"Green\"]`\n",
    "   - `[\"USA\", \"Germany\", \"Japan\"]`\n",
    "   - `[\"Apple\", \"Banana\", \"Orange\"]`\n",
    "\n",
    "   For nominal data, **any numerical assignment must avoid implying ordering**. Encoding \"Red\" as 1, \"Blue\" as 2, and \"Green\" as 3 would misleadingly suggest that Green > Blue > Red—a false ordinal relationship.\n",
    "\n",
    "2. **Ordinal Variables**:  \n",
    "   These categories **do have a meaningful order**, though the distances between levels may not be uniform. Examples include:\n",
    "   - `[\"Low\", \"Medium\", \"High\"]`\n",
    "   - `[\"Poor\", \"Fair\", \"Good\", \"Excellent\"]`\n",
    "   - Education levels: `[\"High School\", \"Bachelor\", \"Master\", \"PhD\"]`\n",
    "\n",
    "   Here, the ordering is real and should be preserved in the encoding. However, the numerical mapping must reflect the semantics—not arbitrary integers—unless the algorithm can handle ordinality natively.\n",
    "\n",
    "---\n",
    "\n",
    "### **Encoding Methods: Principles, Trade-offs, and Implementation**\n",
    "\n",
    "Below is a detailed comparison of encoding strategies, followed by in-depth explanations and code demonstrations.\n",
    "\n",
    "| Method | Best For | Pros | Cons | Risk of Data Leakage? |\n",
    "|--------|----------|------|------|------------------------|\n",
    "| **Label Encoding** | Ordinal data | Simple, memory-efficient | Implies order in nominal data | Low |\n",
    "| **One-Hot Encoding** | Low-cardinality nominal data | No false ordering | High dimensionality (curse of dimensionality) | None |\n",
    "| **Ordinal Encoding** | Ordinal data with known mapping | Preserves order explicitly | Requires manual mapping | None |\n",
    "| **Target Encoding** | High-cardinality nominal data | Compact, captures target correlation | High risk of overfitting | **Yes** (if not regularized or cross-validated) |\n",
    "| **Frequency Encoding** | High-cardinality data | Simple, preserves distribution | Loses category identity | Low |\n",
    "| **Binary Encoding** | Medium-to-high cardinality | Reduces dimensions vs. OHE | Introduces artificial groupings | None |\n",
    "| **Embedding (Neural)** | Deep learning with high-cardinality | Learns dense representations | Requires large data & neural nets | None (but needs careful training) |\n",
    "\n",
    "We now explore each method in detail.\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Label Encoding**\n",
    "\n",
    "**Concept**: Assign a unique integer to each category (e.g., \"Red\" → 0, \"Blue\" → 1, \"Green\" → 2).\n",
    "\n",
    "**When to Use**: **Only for ordinal variables** where ordering is meaningful.\n",
    "\n",
    "**Caution**: Never use for nominal variables in models sensitive to magnitude (e.g., linear regression, SVM, k-NN), as it introduces false ordinal bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "973c2397-aa4e-4b1b-90bd-aa133d7b3dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     size  size_encoded  size_le\n",
      "0   Small             0        2\n",
      "1  Medium             1        1\n",
      "2   Large             2        0\n",
      "3   Small             0        2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Example: Ordinal data\n",
    "df_size = pd.DataFrame({'size': ['Small', 'Medium', 'Large', 'Small']})\n",
    "\n",
    "# Define the correct order\n",
    "size_order = {'Small': 0, 'Medium': 1, 'Large': 2}\n",
    "df_size['size_encoded'] = df_size['size'].map(size_order)\n",
    "\n",
    "# Alternatively, if using LabelEncoder (less safe for nominal):\n",
    "le = LabelEncoder()\n",
    "df_size['size_le'] = le.fit_transform(df_size['size'])\n",
    "# But note: LabelEncoder assigns alphabetically—'Large'=1, 'Medium'=2, 'Small'=0 → WRONG ORDER!\n",
    "\n",
    "print(df_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca26d8-a1f6-430d-800e-3adcd2c2a0b4",
   "metadata": {},
   "source": [
    "> **Best Practice**: For ordinal data, **manually map categories to integers** based on domain knowledge—not automatic label encoding.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. One-Hot Encoding (OHE)**\n",
    "\n",
    "**Concept**: Replace a categorical column with **k binary columns** (for k categories), where only one is \"hot\" (1) per row.\n",
    "\n",
    "**When to Use**: **Nominal variables with low to moderate cardinality** (e.g., < 10–15 categories). Ideal for tree-based models (Random Forest, XGBoost) and linear models that can't infer category relationships.\n",
    "\n",
    "**Drawback**: For high-cardinality features (e.g., ZIP codes with 40,000 values), OHE creates **thousands of sparse columns**, leading to memory issues and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "120dde88-7fa4-46c0-ba8c-da5b452b2b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded DataFrame:\n",
      "    color_green  color_red  color_yellow\n",
      "0          0.0        1.0           0.0\n",
      "1          0.0        0.0           0.0\n",
      "2          0.0        1.0           0.0\n",
      "3          1.0        0.0           0.0\n",
      "4          0.0        0.0           1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_color = pd.DataFrame({'color': ['red', 'blue', 'red', 'green', 'yellow']})\n",
    "\n",
    "# Use sparse=False for readability (in practice, keep sparse=True for memory)\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')  # drop='first' avoids multicollinearity\n",
    "encoded = encoder.fit_transform(df_color[['color']])\n",
    "feature_names = encoder.get_feature_names_out(['color'])\n",
    "encoded_df = pd.DataFrame(encoded, columns=feature_names)\n",
    "\n",
    "print(\"Encoded DataFrame:\\n\", encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b188b3-323d-490c-8537-4bc6a47b9b11",
   "metadata": {},
   "source": [
    "> **Note on `drop='first'`**: In linear models, including all k dummies creates perfect multicollinearity (the \"dummy variable trap\"). Dropping one category resolves this.\n",
    "\n",
    "**Advanced Tip**: Use `pandas.get_dummies()` for quick prototyping, but prefer `sklearn`'s `OneHotEncoder` in production pipelines for consistency and integration with `ColumnTransformer`.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Ordinal Encoding**\n",
    "\n",
    "Unlike label encoding (which is arbitrary), **ordinal encoding** uses a **predefined mapping** that reflects true category order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cacfe4b-89b9-46da-8c23-f2ddba138a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     education  education_ordinal\n",
      "0     Bachelor                1.0\n",
      "1          PhD                3.0\n",
      "2  High School                0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Explicitly define the order\n",
    "education_levels = ['High School', 'Bachelor', 'Master', 'PhD']\n",
    "oe = OrdinalEncoder(categories=[education_levels])\n",
    "\n",
    "df_edu = pd.DataFrame({'education': ['Bachelor', 'PhD', 'High School']})\n",
    "df_edu['education_ordinal'] = oe.fit_transform(df_edu[['education']])\n",
    "\n",
    "print(df_edu)\n",
    "# Output: Bachelor → 1, PhD → 3, High School → 0 → CORRECT ORDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b81bf3c-01be-4b43-a7a0-ab71d64dce46",
   "metadata": {},
   "source": [
    "This is the **gold standard for ordinal variables** in sklearn-compatible workflows.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Target Encoding (Mean Encoding)**\n",
    "\n",
    "**Concept**: Replace each category with the **mean of the target variable** for that category.\n",
    "\n",
    "Example: In a house price prediction task, encode \"Neighborhood\" as the average price of houses in that neighborhood.\n",
    "\n",
    "**Formula**:\n",
    "\\[\n",
    "\\text{Encoded}(c) = \\frac{\\sum_{i:y_i \\in c} y_i}{n_c}\n",
    "\\]\n",
    "\n",
    "**When to Use**: **High-cardinality nominal features** (e.g., user IDs, product SKUs, ZIP codes) where OHE is infeasible.\n",
    "\n",
    "**Pitfall**: **Severe overfitting** if applied naively. A rare category with one sample will encode to that sample’s exact target value—perfect memorization!\n",
    "\n",
    "**Solution**: **Regularized target encoding** or **cross-validated encoding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93ef320b-f2be-4eea-a8f2-f5fbfbd68ff6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Naive target encoding (DANGEROUS!)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneighborhood_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneighborhood\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Safe approach: Use cross-validation or smoothing\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcategory_encoders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TargetEncoder\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Naive target encoding (DANGEROUS!)\n",
    "df['neighborhood_encoded'] = df.groupby('neighborhood')['price'].transform('mean')\n",
    "\n",
    "# Safe approach: Use cross-validation or smoothing\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Simulate regression target\n",
    "df_house = pd.DataFrame({\n",
    "    'neighborhood': ['A', 'B', 'A', 'C', 'B', 'A', 'C', 'C'],\n",
    "    'price': [300, 500, 320, 700, 490, 310, 710, 690]\n",
    "})\n",
    "\n",
    "# TargetEncoder applies smoothing and CV internally\n",
    "te = TargetEncoder()\n",
    "df_house['neighborhood_te'] = te.fit_transform(df_house['neighborhood'], df_house['price'])\n",
    "\n",
    "print(df_house[['neighborhood', 'price', 'neighborhood_te']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a616cca9-4947-4d2a-892b-2e3361e6db8b",
   "metadata": {},
   "source": [
    "> **Library Recommendation**: Use `category_encoders` (a third-party library) for robust, production-ready target encoding with smoothing, cross-validation, and noise injection.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Frequency Encoding**\n",
    "\n",
    "Replace each category with its **observed frequency** in the dataset.\n",
    "\n",
    "**Use Case**: When category frequency is predictive (e.g., rare words in NLP may signal spam).\n",
    "\n",
    "```python\n",
    "freq_map = df['category'].value_counts().to_dict()\n",
    "df['category_freq'] = df['category'].map(freq_map)\n",
    "```\n",
    "\n",
    "**Limitation**: Two different categories with the same frequency become indistinguishable.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Binary Encoding**\n",
    "\n",
    "Combines OHE and dimensionality reduction:\n",
    "1. Label-encode categories → integers.\n",
    "2. Convert integers to binary.\n",
    "3. Split binary digits into separate columns.\n",
    "\n",
    "For 8 categories → only 3 binary columns (vs. 8 in OHE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17577f2b-de75-416a-8043-4028e775bcac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'category_encoders'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcategory_encoders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BinaryEncoder\n\u001b[0;32m      3\u001b[0m df_id \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m101\u001b[39m, \u001b[38;5;241m102\u001b[39m, \u001b[38;5;241m103\u001b[39m, \u001b[38;5;241m104\u001b[39m]})\n\u001b[0;32m      4\u001b[0m be \u001b[38;5;241m=\u001b[39m BinaryEncoder(cols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'category_encoders'"
     ]
    }
   ],
   "source": [
    "from category_encoders import BinaryEncoder\n",
    "\n",
    "df_id = pd.DataFrame({'user_id': [101, 102, 103, 104]})\n",
    "be = BinaryEncoder(cols=['user_id'])\n",
    "df_encoded = be.fit_transform(df_id)\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7a6b06-9237-405c-b468-361139b445b1",
   "metadata": {},
   "source": [
    "Useful for **medium-cardinality** features (e.g., 100–10,000 categories).\n",
    "\n",
    "---\n",
    "\n",
    "#### **7. Embedding Layers (Deep Learning)**\n",
    "\n",
    "In neural networks, categorical variables can be mapped to **dense, low-dimensional vectors** (embeddings) learned during training.\n",
    "\n",
    "Example (using TensorFlow/Keras):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abec84da-e15c-464a-b2b7-ed09eb9fbc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Suppose 1000 unique categories, embed into 50 dimensions\n",
    "vocab_size = 1000\n",
    "embedding_dim = 50\n",
    "\n",
    "input_cat = Input(shape=(1,), name='category_input')\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_cat)\n",
    "flat_emb = Flatten()(embedding)\n",
    "\n",
    "# Rest of the model...\n",
    "model = Model(inputs=input_cat, outputs=flat_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db22dabe-08d7-4922-8b00-5bd48e7e2d29",
   "metadata": {},
   "source": [
    "Embeddings capture **semantic similarity**: similar categories (e.g., \"Paris\" and \"London\") end up with nearby vectors.\n",
    "\n",
    "---\n",
    "\n",
    "### **Choosing the Right Encoder: Decision Flowchart**\n",
    "\n",
    "1. **Is the variable ordinal?**  \n",
    "   → Yes: Use **Ordinal Encoding** with explicit order.  \n",
    "   → No: Proceed.\n",
    "\n",
    "2. **How many unique categories?**  \n",
    "   - **< 10**: **One-Hot Encoding** (with `drop='first'` for linear models).  \n",
    "   - **10–100**: Consider **Binary Encoding** or **Frequency Encoding**.  \n",
    "   - **> 100**: **Target Encoding** (with regularization) or **Embeddings** (if using deep learning).\n",
    "\n",
    "3. **Is the target available during preprocessing?**  \n",
    "   → Only during training! Never encode test data using target stats from the full dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **Critical Pitfall: Data Leakage in Encoding**\n",
    "\n",
    "**Never fit an encoder on the entire dataset before splitting**. Always:\n",
    "1. Split data into train/test.\n",
    "2. **Fit encoder only on training data**.\n",
    "3. **Transform both train and test using the fitted encoder**.\n",
    "\n",
    "Example of **correct pipeline**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df4a00e-14b5-48bf-8c2b-d16ce5578c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "X = df[['color', 'size', 'price']]\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing for categorical columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(drop='first'), ['color', 'size'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Fit ONLY on training data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)  # No fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7ddd7d-68bd-4e48-a710-9533e3ac37bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
