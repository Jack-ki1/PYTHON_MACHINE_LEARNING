{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "688a2476-9dd9-454f-81b2-b4cd2df934d7",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#fbbc05;\">2.1 Collecting Data: The First Step in Any ML Project</h3>\n",
    "\n",
    "Before you can build a model, you need data. In industry, data comes from three primary sources, each with its own considerations, advantages, and challenges.\n",
    "\n",
    "<h4 style=\"color:#1a73e8;\">2.1.1 Open-Source Datasets</h4>\n",
    "\n",
    "These are invaluable for learning, prototyping, and benchmarking. They're free, well-documented, and often come with example code.\n",
    "\n",
    "**Major Sources**:\n",
    "\n",
    "1. **Kaggle Datasets** (`kaggle.com/datasets`)\n",
    "   - **What it is**: The world's largest data science community with millions of datasets\n",
    "   - **Types**: Everything from Titanic passenger lists to satellite imagery to financial data\n",
    "   - **Advantages**: \n",
    "     - Datasets are often cleaned and documented\n",
    "     - Community discussions and kernels (example code)\n",
    "     - Competitions provide real-world problems\n",
    "   - **Industry Example**: A fintech startup prototypes a credit risk model using the \"Give Me Some Credit\" dataset before accessing their proprietary user data. This allows them to test algorithms and validate approaches without risking sensitive customer information.\n",
    "\n",
    "2. **UCI Machine Learning Repository** (`archive.ics.uci.edu/ml`)\n",
    "   - **What it is**: A long-standing academic repository maintained by UC Irvine\n",
    "   - **Types**: Classic datasets used in research papers (Iris, Wine, Adult Income, etc.)\n",
    "   - **Advantages**: \n",
    "     - Well-documented with metadata\n",
    "     - Used in research, so results are comparable\n",
    "     - Clean, structured format\n",
    "   - **Industry Example**: Researchers at a hospital use the UCI \"Heart Disease\" dataset to validate a new diagnostic feature before clinical trials. This helps them understand data requirements and expected performance.\n",
    "\n",
    "3. **Government & Public Data**\n",
    "   - **USA**: `data.gov` (housing, climate, economic indicators, transportation)\n",
    "   - **EU**: `data.europa.eu` (European Union open data)\n",
    "   - **UK**: `data.gov.uk` (UK government data)\n",
    "   - **World Bank**: `data.worldbank.org` (global economic and social data)\n",
    "   - **Advantages**: \n",
    "     - Real-world, often large-scale data\n",
    "     - Updated regularly\n",
    "     - Free and legally safe to use\n",
    "   - **Industry Example**: A logistics company uses U.S. Department of Transportation data to predict highway congestion. This real-time data helps them optimize delivery routes and reduce fuel costs.\n",
    "\n",
    "**Downloading from Kaggle**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f9a687-1808-4f3b-82ef-02015d403e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Kaggle API: pip install kaggle\n",
    "# Get API credentials from: https://www.kaggle.com/account\n",
    "\n",
    "import kaggle\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Download a dataset\n",
    "api.dataset_download_files('dataset-name', path='./data', unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4041df-8cd9-4535-9c52-21ee7717f82e",
   "metadata": {},
   "source": [
    "<h4 style=\"color:#1a73e8;\">2.1.2 APIs (Application Programming Interfaces)</h4>\n",
    "\n",
    "APIs provide structured, programmatic access to live or regularly updated data. They're essential for production ML systems that need real-time or frequently refreshed data.\n",
    "\n",
    "**What is an API?** An API is a way for different software systems to communicate. In data collection, APIs let you request data from a server and receive it in a structured format (usually JSON).\n",
    "\n",
    "**Common API Use Cases**:\n",
    "- **Weather data**: For agricultural yield prediction, energy demand forecasting\n",
    "- **Financial data**: Stock prices, exchange rates, economic indicators\n",
    "- **Social media**: Twitter, Reddit (with rate limits and terms of service)\n",
    "- **E-commerce**: Product prices, reviews, inventory levels\n",
    "- **Government**: Census data, employment statistics\n",
    "\n",
    "**Example: Fetching Weather Data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f8fdc2-af3b-4da3-a34e-59ef7dbd2852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Get your free API key from https://openweathermap.org/api\n",
    "# Store it as an environment variable (never hardcode!)\n",
    "API_KEY = os.getenv('OPENWEATHER_API_KEY')  # Set in your system or .env file\n",
    "CITY = \"London\"\n",
    "\n",
    "# Construct the API URL\n",
    "url = f\"http://api.openweathermap.org/data/2.5/weather?q={CITY}&appid={API_KEY}&units=metric\"\n",
    "\n",
    "try:\n",
    "    # Make the request\n",
    "    response = requests.get(url, timeout=10)  # 10 second timeout\n",
    "    \n",
    "    # Check if request was successful\n",
    "    if response.status_code == 200:\n",
    "        weather_data = response.json()\n",
    "        \n",
    "        # Extract relevant information\n",
    "        temperature = weather_data['main']['temp']\n",
    "        humidity = weather_data['main']['humidity']\n",
    "        description = weather_data['weather'][0]['description']\n",
    "        \n",
    "        print(f\"Current weather in {CITY}:\")\n",
    "        print(f\"  Temperature: {temperature}Â°C\")\n",
    "        print(f\"  Humidity: {humidity}%\")\n",
    "        print(f\"  Conditions: {description}\")\n",
    "        \n",
    "        # Save to DataFrame for ML use\n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame([{\n",
    "            'city': CITY,\n",
    "            'timestamp': datetime.now(),\n",
    "            'temperature': temperature,\n",
    "            'humidity': humidity,\n",
    "            'description': description\n",
    "        }])\n",
    "        \n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(f\"Message: {response.text}\")\n",
    "        \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Request failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b0d6d-e77b-4f48-8440-e5fa6a31cb32",
   "metadata": {},
   "source": [
    "**Best Practices for API Usage**:\n",
    "1. **Rate Limiting**: Respect API rate limits. Add delays between requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef4885-39de-4e20-8aff-754e37ac0b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(1)  # Wait 1 second between requests\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e2b876-c80f-44f3-aae6-4a4044e208a1",
   "metadata": {},
   "source": [
    "2. **Error Handling**: Always handle errors gracefully: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de191bf-9bd5-4c1e-b5a1-e055a9fefc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "       response = requests.get(url)\n",
    "       response.raise_for_status()  # Raises exception for bad status codes\n",
    "   except requests.exceptions.HTTPError as e:\n",
    "       print(f\"HTTP error: {e}\")\n",
    "   except requests.exceptions.RequestException as e:\n",
    "       print(f\"Request error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b0b1e-7f1e-44f7-bb61-0bfa7f124a46",
   "metadata": {},
   "source": [
    "3. **Authentication**: Never hardcode API keys. Use environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac30ca85-e152-439f-ab1d-0dce4cfdccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a .env file (add to .gitignore!)\n",
    "# OPENWEATHER_API_KEY=your_key_here\n",
    "   \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv('OPENWEATHER_API_KEY') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc980302-aebb-4a53-87d4-f65342982f36",
   "metadata": {},
   "source": [
    "4. **Caching**: Cache API responses to avoid unnecessary requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd9f058-80fc-47fa-8770-b217c27d5098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import hashlib\n",
    "   \n",
    "       def get_cached_data(url, cache_dir='cache'):\n",
    "           # Create hash of URL as cache key\n",
    "           cache_key = hashlib.md5(url.encode()).hexdigest()\n",
    "           cache_path = f\"{cache_dir}/{cache_key}.pkl\"\n",
    "           \n",
    "           if os.path.exists(cache_path):\n",
    "               with open(cache_path, 'rb') as f:\n",
    "                   return pickle.load(f)\n",
    "           else:\n",
    "               response = requests.get(url)\n",
    "               data = response.json()\n",
    "               os.makedirs(cache_dir, exist_ok=True)\n",
    "               with open(cache_path, 'wb') as f:\n",
    "                   pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfdd640-6a4c-4c5b-a9c1-33348b40e259",
   "metadata": {},
   "source": [
    "> **Security Note**: Never commit API keys to version control. Use environment variables or secure key management services (AWS Secrets Manager, Azure Key Vault) in production.\n",
    "\n",
    "<h4 style=\"color:#1a73e8;\">2.1.3 Web Scraping (Use with Extreme Caution)</h4>\n",
    "\n",
    "Web scraping involves programmatically extracting data from websites. This is a **legal and ethical gray area** that requires careful consideration.\n",
    "\n",
    "**When Scraping is Acceptable**:\n",
    "- The website's `robots.txt` file permits it (check `website.com/robots.txt`)\n",
    "- The data is publicly available and not behind authentication\n",
    "- You're not overloading their servers (add delays between requests)\n",
    "- The data is not protected by copyright or terms of service\n",
    "- You're using the data for research or personal learning (not commercial use without permission)\n",
    "\n",
    "**When to Avoid Scraping**:\n",
    "- The website explicitly prohibits it in their Terms of Service\n",
    "- The data requires authentication (login)\n",
    "- You're scraping at a rate that could harm the website\n",
    "- The data is copyrighted or proprietary\n",
    "- You plan to use it commercially without permission\n",
    "\n",
    "**Basic Web Scraping Example** (Educational Only):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5fe1e-88e8-4bf4-b8e1-2565eabe9cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Always check robots.txt first!\n",
    "# Example: https://example.com/robots.txt\n",
    "\n",
    "def scrape_with_respect(url, delay=2):\n",
    "    \"\"\"\n",
    "    Scrape a webpage with respect for the server.\n",
    "    \n",
    "    Parameters:\n",
    "    - url: The webpage to scrape\n",
    "    - delay: Seconds to wait between requests (be respectful!)\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Educational Bot)'  # Identify yourself\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract data (example: finding all links)\n",
    "        links = []\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            links.append(link['href'])\n",
    "        \n",
    "        # Be respectful: wait before next request\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        return links\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage (only for educational purposes!)\n",
    "# data = scrape_with_respect('https://example.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23d8bbe-cbfa-4e7f-adce-8d65dbeeb5bb",
   "metadata": {},
   "source": [
    "**Industry Context**: An e-commerce company monitors competitor pricing by scraping public product pages. This is a common practice, though legally sensitive. Many companies use specialized services (like Price2Spy) that have agreements with retailers, rather than scraping directly.\n",
    "\n",
    "> **Golden Rule**: **Always prioritize data provenance and ethics**. Biased or illegally obtained data can lead to model failure, reputational damage, or legal liability. When in doubt, use official APIs or purchase data from legitimate providers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bbb70c-c019-476d-bf95-8783b0c99dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
