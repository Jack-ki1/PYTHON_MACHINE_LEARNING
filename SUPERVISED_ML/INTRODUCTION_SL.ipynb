{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed87c6c-c528-4d45-8595-7f9cb7460d09",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c0493d4-c122-474f-9a7d-94d8720c5b86",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#1a73e8; border-bottom:2px solid #ea4335; padding-bottom:0.2em;\">PART 3: SUPERVISED LEARNING</h2>\n",
    "\n",
    "> **“All models are wrong, but some are useful.”**  \n",
    "> — George E. P. Box\n",
    "\n",
    "Supervised learning is the cornerstone of applied machine learning. It is the task of learning a mapping from inputs **X** to outputs **y** from a set of labeled examples **{(x₁, y₁), ..., (xₘ, yₘ)}**. This part will transform you from a data preparer into a model builder. We will explore the **mathematical soul** of each algorithm, its **practical intuition**, its **strengths and weaknesses**, and its **real-world application**.\n",
    "\n",
    "This chapter is structured around a core philosophy: **understanding before implementation**. For each of the 10 key algorithms, we will first build a deep conceptual foundation. Only then will we move to a comprehensive, production-grade Python implementation.\n",
    "\n",
    "Our journey begins with the fundamental concepts that underpin all supervised learning.\n",
    "\n",
    "---\n",
    "\n",
    "<h3 style=\"color:#fbbc05;\">3.1 Introduction to Supervised Learning</h3>\n",
    "\n",
    "\n",
    "<span style=\"color:#ea4335; font-weight:bold;\">The Core Problem</span>\n",
    "\n",
    "Given a dataset of **m** examples, where each example has **n** input features **x = [x₁, x₂, ..., xₙ]** and a target output **y**, the goal is to learn a hypothesis function **h(x; θ)** that can accurately predict **y** for new, unseen **x**.\n",
    "\n",
    "- **Regression**: **y** is a continuous real number (e.g., house price, temperature).\n",
    "- **Classification**: **y** is a discrete class label (e.g., spam/ham, dog/cat).\n",
    "\n",
    "The learned function **h** is parameterized by a vector **θ** (e.g., weights and biases).\n",
    "\n",
    "\n",
    "<span style=\"color:#ea4335; font-weight:bold;\">The Bias-Variance Tradeoff: The Fundamental Tension</span>\n",
    "\n",
    "Every model’s prediction error can be decomposed into three parts:\n",
    "\\[\n",
    "\\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}\n",
    "\\]\n",
    "\n",
    "- **Bias**: Error from erroneous assumptions in the learning algorithm. A high-bias model is **too simple** to capture the underlying pattern (underfitting).\n",
    "- **Variance**: Error from sensitivity to small fluctuations in the training set. A high-variance model is **too complex** and fits the noise (overfitting).\n",
    "- **Irreducible Error**: Noise inherent in the problem itself (e.g., measurement error).\n",
    "\n",
    "> **Figure 3.1**: A visual representation of the tradeoff. On the left, a high-bias (linear) model misses the true curve. On the right, a high-variance (15th-degree polynomial) model fits every training point but oscillates wildly elsewhere. The middle shows the optimal balance.\n",
    "\n",
    "The art of machine learning is to find the model complexity that minimizes the total error.\n",
    "\n",
    "---\n",
    "\n",
    "<h3 style=\"color:#fbbc05;\">3.2 Mathematical Foundations</h3>\n",
    "\n",
    "\n",
    "<span style=\"color:#ea4335; font-weight:bold;\">3.2.1 Loss Functions: Measuring Prediction Error</span>\n",
    "\n",
    "A loss function **L(y, ŷ)** quantifies how bad a single prediction **ŷ** is compared to the true value **y**.\n",
    "\n",
    "- **For Regression: Mean Squared Error (MSE)**\n",
    "  \\[\n",
    "  L(y, ŷ) = (y - ŷ)^2\n",
    "  \\]\n",
    "  The total cost over the entire dataset is the average:\n",
    "  \\[\n",
    "  J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} (y^{(i)} - h(x^{(i)}; \\theta))^2\n",
    "  \\]\n",
    "  MSE penalizes large errors heavily (squared term), making it sensitive to outliers.\n",
    "\n",
    "- **For Classification: Log Loss (Cross-Entropy)**\n",
    "  For binary classification where **y ∈ {0, 1}** and the model outputs a probability **p = P(y=1|x)**:\n",
    "  \\[\n",
    "  L(y, p) = -[y \\log(p) + (1 - y) \\log(1 - p)]\n",
    "  \\]\n",
    "  The intuition is: if the true label is **1**, we want **p** to be close to **1**, so **-log(p)** will be small. If the model is confident and wrong (e.g., **y=1** but **p=0.01**), the loss becomes very large.\n",
    "\n",
    "\n",
    "<span style=\"color:#ea4335; font-weight:bold;\">3.2.2 Optimization: Gradient Descent</span>\n",
    "\n",
    "To find the best parameters **θ**, we minimize the cost function **J(θ)**. The most common method is **Gradient Descent**.\n",
    "\n",
    "The gradient **∇J(θ)** is a vector of partial derivatives, pointing in the direction of steepest ascent of **J**. To minimize **J**, we move in the opposite direction.\n",
    "\\[\n",
    "\\theta := \\theta - \\alpha \\nabla J(\\theta)\n",
    "\\]\n",
    "where **α** is the **learning rate**, a crucial hyperparameter that controls the step size.\n",
    "\n",
    "> **Derivation of the MSE Gradient** (for Linear Regression):  \n",
    "> The hypothesis is **h(x; θ) = θ₀ + θ₁x₁ + ... + θₙxₙ = θᵀx** (with **x₀ = 1**).  \n",
    "> The partial derivative with respect to a single parameter **θⱼ** is:\n",
    "> \\[\n",
    "> \\frac{\\partial J}{\\partial \\theta_j} = \\frac{2}{m} \\sum_{i=1}^{m} (h(x^{(i)}; \\theta) - y^{(i)}) x_j^{(i)}\n",
    "> \\]\n",
    "> This gives us a recipe to update every **θⱼ**.\n",
    "\n",
    "---\n",
    "\n",
    "<h3 style=\"color:#fbbc05;\">3.3 The 10 Key Supervised Learning Algorithms</h3>\n",
    "\n",
    "Now, we dive into the algorithms themselves. Each section follows this structure:\n",
    "\n",
    "1. **Intuition & Use Case**\n",
    "2. **Mathematical Formulation**\n",
    "3. **Pros & Cons**\n",
    "4. **Comprehensive Python Implementation** (40+ lines, fully commented)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
